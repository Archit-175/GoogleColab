{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEoBSynMTKk0dS9eZ3lfyi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Archit-175/GoogleColab/blob/main/Stemming_Vs_Lemmatization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming** \\\n",
        "Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Stemming is important in natural language understanding (NLU) and natural language processing (NLP)."
      ],
      "metadata": {
        "id": "2bm_-IafieUY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oHuuhQXSiDlr"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming = PorterStemmer()"
      ],
      "metadata": {
        "id": "IG4pEbD4jmdx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words=[\"eating\",\"eats\",\"ate\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]\n"
      ],
      "metadata": {
        "id": "3a2542_JjtTZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(word+\"---->\"+stemming.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HzHY5x7kBxC",
        "outputId": "dd93d862-ed91-4a20-ec74-0b10691a87f4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "ate---->ate\n",
            "eaten---->eaten\n",
            "writing---->write\n",
            "writes---->write\n",
            "programming---->program\n",
            "programs---->program\n",
            "history---->histori\n",
            "finally---->final\n",
            "finalized---->final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem('congratulations')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cztyfjQIkL09",
        "outputId": "0b0cb5d6-839a-45f8-f4ef-3b45dcd24a00"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'congratul'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem('sitting')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3EHU6c1GkYTp",
        "outputId": "486e24d9-9c26-47e3-de47-04fe4b0c5f85"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sit'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lancaster Stemming Algorithm"
      ],
      "metadata": {
        "id": "fTC3pw-DkjiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer"
      ],
      "metadata": {
        "id": "J9dKqtvFkdg8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lancaster = LancasterStemmer()"
      ],
      "metadata": {
        "id": "tCyzNZVjktQK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(word+\"---->\"+lancaster.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrYd7_O0kvPn",
        "outputId": "7f5b3c9e-c743-4991-f0d8-9c6e42b340be"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "ate---->at\n",
            "eaten---->eat\n",
            "writing---->writ\n",
            "writes---->writ\n",
            "programming---->program\n",
            "programs---->program\n",
            "history---->hist\n",
            "finally---->fin\n",
            "finalized---->fin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RegexpStemmer class**\\\n",
        "NLTK has RegexpStemmer class with the help of which we can easily implement Regular Expression Stemmer algorithms. It basically takes a single regular expression and removes any prefix or suffix that matches the expression. Let us see an example"
      ],
      "metadata": {
        "id": "JF2Kl3fmk-LG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer"
      ],
      "metadata": {
        "id": "FnCNW4Fakxgv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer=RegexpStemmer('ing$|s$|e$|able$', min=4)"
      ],
      "metadata": {
        "id": "yOc__K70lGnG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer.stem('eating')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2LOQA7EUlQ3l",
        "outputId": "0323483a-24a3-427b-8b46-a1fab703ab26"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer.stem('ingplaying')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zxhxBC4glSgE",
        "outputId": "2f855d57-740a-4eca-96a7-a9af0f3b48d2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ingplay'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer1=RegexpStemmer('ing|s$|e$|able$', min=4)"
      ],
      "metadata": {
        "id": "pT4g6hdvlXS1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer1.stem('ingplaying')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tgGjrbnuliXL",
        "outputId": "05a4d7d9-aaf1-474f-cc22-2f4aa2c569e9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'play'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Snowball Stemmer**"
      ],
      "metadata": {
        "id": "66MkVJyelytw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "rOCXu7P2liwF"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowballstemmer=SnowballStemmer('english',ignore_stopwords=False)"
      ],
      "metadata": {
        "id": "z3Ym3fzzlpYa"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(word+\"---->\"+snowballstemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgZxoe0Zlp0k",
        "outputId": "1f7e909c-d73c-45bc-aca4-2541245f8299"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "ate---->ate\n",
            "eaten---->eaten\n",
            "writing---->write\n",
            "writes---->write\n",
            "programming---->program\n",
            "programs---->program\n",
            "history---->histori\n",
            "finally---->final\n",
            "finalized---->final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem('fairly'),stemming.stem('sportingly')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K5dVRMDmMYJ",
        "outputId": "76c077b0-0e0d-4744-863e-d2fc1ae9322a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fairli', 'sportingli')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snowballstemmer.stem('fairly'),snowballstemmer.stem('sportingly')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CdEeb6ImU1S",
        "outputId": "603ad96f-1e2c-42b7-d8c8-c3de79de8494"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fair', 'sport')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Wordnet Lemmatizer**\\\n",
        "Lemmatization technique is like stemming. The output we will get after lemmatization is called ‘lemma’, which is a root word rather than root stem, the output of stemming. After lemmatization, we will be getting a valid word that means the same thing.\n",
        "\n",
        "NLTK provides WordNetLemmatizer class which is a thin wrapper around the wordnet corpus. This class uses morphy() function to the WordNet CorpusReader class to find a lemma. Let us understand it with an example −"
      ],
      "metadata": {
        "id": "Iyqo60qRmb-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BE81OnI-mYs4",
        "outputId": "5e115227-e642-4e4e-9058-7b291e06798f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "POS- Noun-n\n",
        "verb-v\n",
        "adjective-a\n",
        "adverb-r\n",
        "'''\n",
        "for word in words:\n",
        "    print(word+\"---->\"+lemmatizer.lemmatize(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzEVlgm9nTEA",
        "outputId": "a4818238-89bf-4d3c-f231-b866f413badd"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eating\n",
            "eats---->eats\n",
            "ate---->ate\n",
            "eaten---->eaten\n",
            "writing---->writing\n",
            "writes---->writes\n",
            "programming---->programming\n",
            "programs---->program\n",
            "history---->history\n",
            "finally---->finally\n",
            "finalized---->finalized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(word+\"---->\"+lemmatizer.lemmatize(word,pos='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BgC6qHWnWj1",
        "outputId": "4679e3a2-f607-43e6-ee59-f41e1b4a000e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "ate---->eat\n",
            "eaten---->eat\n",
            "writing---->write\n",
            "writes---->write\n",
            "programming---->program\n",
            "programs---->program\n",
            "history---->history\n",
            "finally---->finally\n",
            "finalized---->finalize\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('better',pos='v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "00Be6zFjnz3Z",
        "outputId": "dc0bef5c-5b94-4d7a-ee16-5d45d9ce8c24"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'better'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentiment Analysis** -- stemming \\\n",
        "**Chatbot** -- Lemmatization"
      ],
      "metadata": {
        "id": "HhbQ_F5GoE-u"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XUa4zgd9oDzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jN8e4hmUn79t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}